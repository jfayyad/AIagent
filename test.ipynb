{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f45e8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 12:58:01.790 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-23 12:58:01.791 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-23 12:58:01.792 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-23 12:58:01.792 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     38\u001b[39m         st.warning(\u001b[33m\"\u001b[39m\u001b[33mNeo4j not supported in this Graphiti version. Using local memory.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m Graphiti(\u001b[33m\"\u001b[39m\u001b[33mgemini-whisper-bot\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m graphiti = \u001b[43mget_graphiti\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmemory_seeded\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m st.session_state:\n\u001b[32m     44\u001b[39m     graphiti.save({\n\u001b[32m     45\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33massistant\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     46\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mI remember you were looking for a badminton shoes.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     47\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m\"\u001b[39m: datetime.now().isoformat()\n\u001b[32m     48\u001b[39m     })\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AIagent/agents/lib/python3.13/site-packages/streamlit/runtime/caching/cache_utils.py:225\u001b[39m, in \u001b[36mCachedFunc.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    222\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    223\u001b[39m         spinner_message = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRunning `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m(...)`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_or_create_cached_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspinner_message\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AIagent/agents/lib/python3.13/site-packages/streamlit/runtime/caching/cache_utils.py:267\u001b[39m, in \u001b[36mCachedFunc._get_or_create_cached_value\u001b[39m\u001b[34m(self, func_args, func_kwargs, spinner_message)\u001b[39m\n\u001b[32m    261\u001b[39m spinner_or_no_context = (\n\u001b[32m    262\u001b[39m     spinner(spinner_message, _cache=\u001b[38;5;28;01mTrue\u001b[39;00m, show_time=\u001b[38;5;28mself\u001b[39m._info.show_time)\n\u001b[32m    263\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m spinner_message \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_nested_cache_function\n\u001b[32m    264\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext()\n\u001b[32m    265\u001b[39m )\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m spinner_or_no_context:\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_cache_miss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AIagent/agents/lib/python3.13/site-packages/streamlit/runtime/caching/cache_utils.py:326\u001b[39m, in \u001b[36mCachedFunc._handle_cache_miss\u001b[39m\u001b[34m(self, cache, value_key, func_args, func_kwargs)\u001b[39m\n\u001b[32m    322\u001b[39m \u001b[38;5;66;03m# We acquired the lock before any other thread. Compute the value!\u001b[39;00m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._info.cached_message_replay_ctx.calling_cached_function(\n\u001b[32m    324\u001b[39m     \u001b[38;5;28mself\u001b[39m._info.func\n\u001b[32m    325\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     computed_value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfunc_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfunc_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[38;5;66;03m# We've computed our value, and now we need to write it back to the cache\u001b[39;00m\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# along with any \"replay messages\" that were generated during value computation.\u001b[39;00m\n\u001b[32m    330\u001b[39m messages = \u001b[38;5;28mself\u001b[39m._info.cached_message_replay_ctx._most_recent_messages\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mget_graphiti\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m([neo4j_url, neo4j_user, neo4j_password]):\n\u001b[32m     28\u001b[39m     st.write(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing Neo4j: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mneo4j_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mneo4j_user\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGraphiti\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[43mneo4j_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[43mneo4j_user\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m        \u001b[49m\u001b[43mneo4j_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     35\u001b[39m     st.write(\u001b[33m\"\u001b[39m\u001b[33mNeo4j credentials missing. Using local memory.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AIagent/agents/lib/python3.13/site-packages/graphiti_core/graphiti.py:176\u001b[39m, in \u001b[36mGraphiti.__init__\u001b[39m\u001b[34m(self, uri, user, password, llm_client, embedder, cross_encoder, store_raw_episode_content, graph_driver, max_coroutines)\u001b[39m\n\u001b[32m    174\u001b[39m     \u001b[38;5;28mself\u001b[39m.llm_client = llm_client\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     \u001b[38;5;28mself\u001b[39m.llm_client = \u001b[43mOpenAIClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m embedder:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28mself\u001b[39m.embedder = embedder\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AIagent/agents/lib/python3.13/site-packages/graphiti_core/llm_client/openai_client.py:59\u001b[39m, in \u001b[36mOpenAIClient.__init__\u001b[39m\u001b[34m(self, config, cache, client, max_tokens)\u001b[39m\n\u001b[32m     56\u001b[39m     config = LLMConfig()\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28mself\u001b[39m.client = \u001b[43mAsyncOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m.client = client\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AIagent/agents/lib/python3.13/site-packages/openai/_client.py:449\u001b[39m, in \u001b[36mAsyncOpenAI.__init__\u001b[39m\u001b[34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[39m\n\u001b[32m    447\u001b[39m     api_key = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[32m    450\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    451\u001b[39m     )\n\u001b[32m    452\u001b[39m \u001b[38;5;28mself\u001b[39m.api_key = api_key\n\u001b[32m    454\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mOpenAIError\u001b[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import streamlit as st\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableMap\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from transformers import pipeline\n",
    "from datetime import datetime\n",
    "from graphiti_core import Graphiti\n",
    "\n",
    "load_dotenv()\n",
    "st.set_page_config(page_title=\"Gemini Whisper Bot\", layout=\"wide\")\n",
    "\n",
    "def get_env_var(key):\n",
    "    if hasattr(st, \"secrets\") and key in st.secrets:\n",
    "        return st.secrets[key]\n",
    "    return os.getenv(key)\n",
    "\n",
    "neo4j_url = get_env_var(\"NEO4J_URL\")\n",
    "neo4j_user = get_env_var(\"NEO4J_USER\")\n",
    "neo4j_password = get_env_var(\"NEO4J_PASSWORD\")\n",
    "\n",
    "@st.cache_resource(show_spinner=False)\n",
    "def get_graphiti():\n",
    "    try:\n",
    "        # Try Neo4j backend (only if your Graphiti version supports it)\n",
    "        if all([neo4j_url, neo4j_user, neo4j_password]):\n",
    "            st.write(f\"Using Neo4j: {neo4j_url} / {neo4j_user}\")\n",
    "            return Graphiti(\n",
    "                neo4j_url,\n",
    "                neo4j_user,\n",
    "                neo4j_password,\n",
    "            )\n",
    "        else:\n",
    "            st.write(\"Neo4j credentials missing. Using local memory.\")\n",
    "            return Graphiti(\"gemini-whisper-bot\")\n",
    "    except TypeError:\n",
    "        st.warning(\"Neo4j not supported in this Graphiti version. Using local memory.\")\n",
    "        return Graphiti(\"gemini-whisper-bot\")\n",
    "\n",
    "graphiti = get_graphiti()\n",
    "\n",
    "if \"memory_seeded\" not in st.session_state:\n",
    "    graphiti.save({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"I remember you were looking for a badminton shoes.\",\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    })\n",
    "    st.session_state.memory_seeded = True\n",
    "\n",
    "@st.cache_resource(show_spinner=False)\n",
    "def get_sentiment_pipeline():\n",
    "    return pipeline(\"sentiment-analysis\")\n",
    "sentiment_pipeline = get_sentiment_pipeline()\n",
    "\n",
    "st.title(\"Gemini Bot with Whispered Guidance\")\n",
    "\n",
    "if \"history\" not in st.session_state:\n",
    "    st.session_state.history = []\n",
    "if \"sentiments\" not in st.session_state:\n",
    "    st.session_state.sentiments = []\n",
    "if \"scores\" not in st.session_state:\n",
    "    st.session_state.scores = []\n",
    "\n",
    "col1, col2 = st.columns(2)\n",
    "\n",
    "with col1:\n",
    "    st.subheader(\"User\")\n",
    "    user_input = st.text_input(\"Type your message:\", key=\"user_input\")\n",
    "    send_button = st.button(\"Send\")\n",
    "\n",
    "with col2:\n",
    "    st.subheader(\"Advisor\")\n",
    "    whisper = st.text_input(\"Whisper to the bot:\", key=\"whisper_input\")\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    try:\n",
    "        result = sentiment_pipeline(text)[0]\n",
    "        label = result[\"label\"]\n",
    "        score = result[\"score\"]\n",
    "        return label, score\n",
    "    except Exception:\n",
    "        return \"NEUTRAL\", 0.5\n",
    "\n",
    "def get_sentiment_display(label, score):\n",
    "    color = {\n",
    "        \"POSITIVE\": \"#d4edda\",\n",
    "        \"NEGATIVE\": \"#f8d7da\",\n",
    "        \"NEUTRAL\": \"#fff3cd\"\n",
    "    }.get(label, \"#f8f9fa\")\n",
    "    return f\"\"\"\n",
    "        <div style=\"background-color:{color};border-radius:8px;padding:6px 12px;display:inline-block;\">\n",
    "        <b>{label.capitalize()}</b> (score: {score:.2f})\n",
    "        </div>\n",
    "    \"\"\"\n",
    "\n",
    "def store_message(role, content):\n",
    "    graphiti.save({\n",
    "        \"role\": role,\n",
    "        \"content\": content,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    })\n",
    "\n",
    "def retrieve_recent_history(n=6):\n",
    "    results = graphiti.search(\"*\", top_k=n, order_by=\"desc\")\n",
    "    messages = [f\"{r['role']}: {r['content']}\" for r in reversed(results)]  # oldest first\n",
    "    return \"\\n\".join(messages)\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0)\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a helpful, warm customer support assistant.\n",
    "\n",
    "You MUST follow any supervisor advice given below exactly and prioritize it in your reply.\n",
    "\n",
    "Context:\n",
    "- Past memory: User has had password trouble on mobile before.\n",
    "- Supervisor advice (optional): {whisper}\n",
    "- User sentiment: {sentiment_label} (score: {sentiment_score:.2f})\n",
    "\n",
    "Recent conversation:\n",
    "{history}\n",
    "\n",
    "User: {user_input}\n",
    "Assistant:\n",
    "\"\"\")\n",
    "\n",
    "if send_button and user_input.strip():\n",
    "    label, score = analyze_sentiment(user_input)\n",
    "    st.session_state.sentiments.append(label)\n",
    "    st.session_state.scores.append(score)\n",
    "\n",
    "    store_message(\"user\", user_input)\n",
    "    full_history = retrieve_recent_history(n=6)\n",
    "\n",
    "    def get_whisper(_): return whisper\n",
    "    def get_user_input(_): return user_input\n",
    "    def get_history(_): return full_history\n",
    "    def get_sentiment_label(_): return label\n",
    "    def get_sentiment_score(_): return score\n",
    "\n",
    "    pipeline_map = (\n",
    "        RunnableMap({\n",
    "            \"whisper\": get_whisper,\n",
    "            \"user_input\": get_user_input,\n",
    "            \"history\": get_history,\n",
    "            \"sentiment_label\": get_sentiment_label,\n",
    "            \"sentiment_score\": get_sentiment_score,\n",
    "        })\n",
    "        | prompt\n",
    "        | llm\n",
    "    )\n",
    "\n",
    "    response = pipeline_map.invoke({})\n",
    "\n",
    "    store_message(\"assistant\", response.content)\n",
    "\n",
    "    st.session_state.history.append(f\"user: {user_input}\")\n",
    "    st.session_state.history.append(f\"assistant: {response.content}\")\n",
    "\n",
    "for i, msg in enumerate(st.session_state.history):\n",
    "    role, content = msg.split(\": \", 1)\n",
    "    with st.chat_message(role):\n",
    "        st.markdown(content)\n",
    "        if role == \"user\":\n",
    "            label = st.session_state.sentiments[i // 2]\n",
    "            score = st.session_state.scores[i // 2]\n",
    "            st.markdown(get_sentiment_display(label, score), unsafe_allow_html=True)\n",
    "            if label == \"NEGATIVE\" and score > 0.7:\n",
    "                st.error(\"Strong negative sentiment detected. Advisor intervention is recommended.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be550cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphiti_core.embedder.gemini import GeminiEmbedder, GeminiEmbedderConfig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
